{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(2.531810426274558)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states = np.arange(0, 4, 0.05)\n",
    "actions = np.arange(0, 4, 0.05)\n",
    "T = 2\n",
    "Q = [np.zeros([round(4 / 0.05), round(4 / 0.05), 2])]\n",
    "mu = [np.ones(len(states)) / len(states)]\n",
    "epsilon = 0.15\n",
    "om_q = 0.55\n",
    "om_mu = 0.85\n",
    "gamma = 0.2\n",
    "rho = 0.95\n",
    "C = 3\n",
    "\n",
    "# given white noise process with specific supports and probabilities\n",
    "supp_W = [0.9, 1.3]\n",
    "pmf_W = [0.75, 0.25]\n",
    "\n",
    "# calculate expectation of white noise process \n",
    "exp_W_gamma = 0\n",
    "for i in range(len(supp_W)):\n",
    "    exp_W_gamma += np.pow(supp_W[i], gamma) * pmf_W[i]\n",
    "\n",
    "# calculate rho * expectation [W ^ gamma]\n",
    "pEWgamma = rho * exp_W_gamma\n",
    "\n",
    "# white noise with two outcomes with probabilities established above\n",
    "def W ():\n",
    "    return np.random.choice(supp_W, p = pmf_W)\n",
    "\n",
    "# given state/action (by index) and mean field investment\n",
    "# return through G(mu, W()) * a (amount invested)\n",
    "# the new state (rounded)\n",
    "# and using formula for utility calculate reward\n",
    "def env (state, action, mu):\n",
    "    consump = states[state] - actions[action]\n",
    "    wealth = actions[action] * W() * C / (pEWgamma * (1 + (C - 1) * np.pow(mu, 3)))\n",
    "    newState = round(20 * wealth)\n",
    "    utility = np.pow(consump, gamma) / gamma\n",
    "    return { 'x': newState, 'u': utility }\n",
    "\n",
    "# rho calculator, given we are on the kth episode and\n",
    "# have visited the specific state/action/time pair count_txa times\n",
    "def rhosCalc (count_txa, k):\n",
    "    rhoQ = 1 / np.pow(1 + count_txa, om_q)\n",
    "    rhoMu = 1 / np.pow(2 + k, om_mu)\n",
    "    return { 'q': rhoQ, 'mu': rhoMu }\n",
    "\n",
    "# eps-greedy policy, takes in 1d array of Q matrix specified by state and time, and state (index)\n",
    "# if Unif[0, 1] > epsilon, choose argmax on 1d array of Q matrix, limited by state\n",
    "# if Unif[0, 1] < epsilon, choose random action, limited by state (unif distribution)\n",
    "def epsAction (Q_x, state):\n",
    "    if np.random.random() > epsilon:\n",
    "        maxim = max(Q_x[:state + 1])\n",
    "        ind = []\n",
    "\n",
    "        for i in range(0, state + 1):\n",
    "            if maxim == Q_x[i]:\n",
    "                ind.append(i)\n",
    "\n",
    "        return actions[ind[np.random.randint(0, len(ind))]]\n",
    "    else:\n",
    "        return np.random.choice(actions[:state + 1])\n",
    "\n",
    "# initialize count for finding rho_Q (learning rate)\n",
    "count = np.zeros([T, len(states), len(actions)])\n",
    "\n",
    "# randomly choose initial state based on unif[0, 1]\n",
    "x = np.random.choice(states[:21])\n",
    "\n",
    "C / (pEWgamma * (1 + (C - 1) * np.pow(0.5, 3)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
