{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ l $ x $ w $ board, spaces enumerated sequentially from $ \\mathbb{N} $, starting at $ NW $ corner and moving right and down like reading a book.\n",
    "\n",
    "$ SE $ spot has reward $ \\texttt{rewar} $, every other space has reward $ \\texttt{cost} $.\n",
    "\n",
    "actions are north, west, south, east, enumerated $ ( 0, 1, 2, 3 ) $, taking at action has probability $ \\texttt{fail} $ to fail and randomly choose any direction to go\n",
    "\n",
    "if agent takes an action that would go off the board it remains in the same space\n",
    "\n",
    "exception is if agent is in goal state (SE), then $ \\mathcal{A} $ corresponds to a teleportation to the four corners of the board $ ( 0 : NE, \\; 1 : NW, \\; 2 : SW, \\; 3 : SE ) $ with the normal fail likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 4\n",
    "height = 4\n",
    "rewar = 1\n",
    "cost = 0\n",
    "fail = 0.1\n",
    "epsilo = 0.1\n",
    "rho = 0.5\n",
    "gamm = 0.8\n",
    "toleran = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "goalState = width * height - 1\n",
    "\n",
    "# reward function that returns reward variable if agent is in the SE space\n",
    "# and return cost variable otherwise\n",
    "def reward (state):\n",
    "    \n",
    "    if state == goalState:\n",
    "        return rewar\n",
    "    else:\n",
    "        return cost\n",
    "\n",
    "# given state is SE-most, check if on the border,\n",
    "# and then get new state or don't change state accordingly\n",
    "# if state is SE-most, then return the corresponding corner state\n",
    "def move (state, choice):\n",
    "\n",
    "    new = state\n",
    "    if state != goalState:\n",
    "        if choice == 0:\n",
    "            if state >= width:\n",
    "                new = state - width\n",
    "        elif choice == 1:\n",
    "            if state % width != 0:\n",
    "                new = state - 1\n",
    "        elif choice == 2:\n",
    "            if state < width * (height - 1):\n",
    "                new = state + width\n",
    "        else:\n",
    "            if state % width != width - 1:\n",
    "                new = state + 1\n",
    "\n",
    "    else:\n",
    "        if choice == 0:\n",
    "            new = width - 1\n",
    "        elif choice == 1:\n",
    "            new = 0\n",
    "        elif choice == 2:\n",
    "            new = width * (height - 1)\n",
    "    return new\n",
    "\n",
    "# simulate if chosen action is taken or fails\n",
    "# if fail randomly generate chosen action\n",
    "def action (state, choice):\n",
    "\n",
    "    if np.random.random() < (1 - fail):\n",
    "        return move (state, choice)\n",
    "    else:\n",
    "        return move (state, np.random.randint(0, 4))\n",
    "\n",
    "# get list of indices of the max value within an 1d numpy array\n",
    "def argQmax (Q_X):\n",
    "\n",
    "    maxim = max(Q_X)\n",
    "    ind = []\n",
    "\n",
    "    for i in range(0, len(Q_X)):\n",
    "        if maxim == Q_X[i]:\n",
    "            ind.append(i)\n",
    "\n",
    "    return ind[np.random.randint(0, len(ind))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-learning method from pseudocode from unified two timescale approach to mean field game/control\n",
    "def QRL (X, A, epsilon, tol_Q, rho_Q, gamma, episodes, ep_len):\n",
    "    \n",
    "    Q = [np.zeros([len(X), len(A)])]\n",
    "\n",
    "    for k in range(1, episodes + 1):\n",
    "        Q.append(Q[k - 1].copy())\n",
    "        state = X[0]\n",
    "        for n in range(0, ep_len):\n",
    "            if np.random.random() > epsilon:\n",
    "                choic = argQmax(Q[k][state])\n",
    "            else:\n",
    "                choic = np.random.randint(0, len(A))\n",
    "            newState = action(state, choic)\n",
    "            r = reward(newState)\n",
    "            # do we need to worry about epsilon-greedy policy for Q values\n",
    "            Q[k][state][choic] += rho_Q * (r + gamma * Q[k][newState][argQmax(Q[k][newState])] - Q[k][state][choic])\n",
    "            state = newState\n",
    "        \n",
    "        if np.absolute(np.subtract(Q[k], Q[k - 1])).sum() < tol_Q:\n",
    "            print(k)\n",
    "            break\n",
    "    \n",
    "    return Q[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the Q Matrix composed of state-action-values\n",
      "\n",
      "               north    west   south    east\n",
      "\n",
      "state   0:     0.979   0.870   0.977   1.095 \n",
      "state   1:     1.022   1.123   1.310   1.357 \n",
      "state   2:     1.546   1.149   1.215   1.744 \n",
      "state   3:     1.568   1.333   2.606   1.493 \n",
      "state   4:     0.897   1.003   1.273   1.090 \n",
      "state   5:     1.119   1.013   1.580   1.462 \n",
      "state   6:     1.318   1.426   1.656   2.018 \n",
      "state   7:     1.685   1.651   3.282   2.159 \n",
      "state   8:     1.056   1.243   1.700   1.199 \n",
      "state   9:     1.400   1.303   1.859   1.615 \n",
      "state  10:     1.738   1.695   3.092   2.097 \n",
      "state  11:     2.223   2.166   3.665   2.595 \n",
      "state  12:     1.332   1.686   1.671   2.159 \n",
      "state  13:     1.840   1.725   2.053   2.673 \n",
      "state  14:     2.095   1.942   2.166   3.618 \n",
      "state  15:     1.744   0.947   1.628   4.384 \n"
     ]
    }
   ],
   "source": [
    "# get value by getting expected value,\n",
    "# considering the epsilon-greedy policy on the actions based on 1d Q_matrix row\n",
    "def getV (Q_X, epsilon):\n",
    "\n",
    "    maxInd = argQmax(Q_X)\n",
    "    v = Q_X[maxInd] * (1 - epsilon)\n",
    "\n",
    "    for i in range(0, 4):\n",
    "        v += Q_X[i] * epsilon / 4\n",
    "\n",
    "    return v\n",
    "\n",
    "# using helper getV method, collect the state-values of each space on the board,\n",
    "# transforming the state space into a 2d list\n",
    "def getVmat (Q_mat, epsilon):\n",
    "    \n",
    "    V = []\n",
    "    for h in range(0, height):\n",
    "        V.append([])\n",
    "        for w in range(0, width):\n",
    "            V[h].append(getV(Q_mat[h * width + w], epsilon))\n",
    "\n",
    "    return V\n",
    "\n",
    "# seed, Q learn, get state-values matrix, print Q-matrix\n",
    "np.random.seed(10)\n",
    "\n",
    "Q_XA = QRL(list(range(0, width * height)), [0, 1, 2, 3], epsilo, toleran, rho, gamm, 100, 1000)\n",
    "V_mat = getVmat(Q_XA, epsilo)\n",
    "\n",
    "print(\"the Q Matrix composed of state-action-values\")\n",
    "print(\"\")\n",
    "print(\"               north    west   south    east\")\n",
    "print(\"\")\n",
    "for i in range(0, width * height):\n",
    "    print(\"state %3d:    %6.3f  %6.3f  %6.3f  %6.3f \" % (i, Q_XA[i][0], Q_XA[i][1], Q_XA[i][2], Q_XA[i][3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "board with calculated state-values on the epsilon-greedy policy\n",
      "\n",
      "+ - - - - + - - - - + - - - - + - - - - +\n",
      "|         |         |         |         |\n",
      "|  1.0833 |  1.3413 |  1.7110 |  2.5200 |\n",
      "|         |         |         |         |\n",
      "+ - - - - + - - - - + - - - - + - - - - +\n",
      "|         |         |         |         |\n",
      "|  1.2519 |  1.5513 |  1.9763 |  3.1728 |\n",
      "|         |         |         |         |\n",
      "+ - - - - + - - - - + - - - - + - - - - +\n",
      "|         |         |         |         |\n",
      "|  1.6595 |  1.8272 |  2.9986 |  3.5646 |\n",
      "|         |         |         |         |\n",
      "+ - - - - + - - - - + - - - - + - - - - +\n",
      "|         |         |         |         |\n",
      "|  2.1148 |  2.6131 |  3.5016 |  4.1635 |\n",
      "|         |         |         |         |\n",
      "+ - - - - + - - - - + - - - - + - - - - +\n"
     ]
    }
   ],
   "source": [
    "print(\"board with calculated state-values on the epsilon-greedy policy\")\n",
    "print(\"\")\n",
    "\n",
    "for h in range(0, height):\n",
    "    for w in range(0, width):\n",
    "        print(\"+ - - - - \", end = '')\n",
    "    print(\"+\")\n",
    "    for w in range(0, width):\n",
    "        print(\"|         \", end = '')\n",
    "    print(\"|\")\n",
    "    for w in range(0, width):\n",
    "        print(\"| %7.4f \" % (V_mat[h][w]), end = '')\n",
    "    print(\"|\")\n",
    "    for w in range(0, width):\n",
    "        print(\"|         \", end = '')\n",
    "    print(\"|\")\n",
    "\n",
    "for w in range(0, width):\n",
    "    print(\"+ - - - - \" % (V_mat[h][w]), end = '')\n",
    "print(\"+\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
